{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "1lULY7mUPG3i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_cVjbaYS44L",
        "outputId": "42e72703-e6c5-44b8-8594-705aacff794f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        "\n",
        "You can download the dataset from [here](https://www.crcv.ucf.edu/research/data-sets/ucf101/) and run this notebook, I have not downloaded the dataset since it is huge (~6GB). But my notebook will most likely work one you have uploaded this dataset to google colab's files space !!"
      ],
      "metadata": {
        "id": "XnywsKCeQGKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_3DCNN(data.Dataset):\n",
        "\n",
        "    '''\n",
        "    folders : stores vidoes in form of images like\n",
        "\n",
        "      - 1\n",
        "          - frame1.jpg\n",
        "          - frame2.jpg\n",
        "          - ...\n",
        "          - frame28.jpg\n",
        "\n",
        "      - 2\n",
        "          - frame1.jpg\n",
        "          - frame2.jpg\n",
        "          - ...\n",
        "          - frame28.jpg\n",
        "\n",
        "      ...\n",
        "\n",
        "    labels : stores class labels for each video\n",
        "    '''\n",
        "\n",
        "    def __init__(self, folders, labels):\n",
        "        self.folders = folders\n",
        "        self.labels = labels\n",
        "        self.transform = transforms.Compose([transforms.Resize([256, 342]), # resizing each frame / image size in the video to 256*342\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.folders)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting that 1 video using the index\n",
        "        folder = self.folders[index]\n",
        "\n",
        "        # Processing on that video\n",
        "        X = []\n",
        "        for i in range(1,29): # cause we are only interested in first 28 frames of the video\n",
        "            image = Image.open(os.path.join(folder, f'frame{i}.jpg')).convert('L')\n",
        "            image = self.transform(image)\n",
        "            X.append(image.squeeze_(0))\n",
        "        X = torch.stack(X, dim=0)\n",
        "        X = X.unsqueeze_(0)\n",
        "\n",
        "        # class information\n",
        "        y = torch.LongTensor([self.labels[index]])\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "H-X3NbGBWjV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = Dataset_3DCNN(folders, labels)\n",
        "train_loader = data.DataLoader(train_set, batch_size = 30, shuffle = True, num_workers = 4, pin_memory = True)\n"
      ],
      "metadata": {
        "id": "X8z8Fdd2QJeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelling**"
      ],
      "metadata": {
        "id": "0MLs-SjQQZ6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3D_output_size(img_size, padding, kernel_size, stride):\n",
        "    return (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
        "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),\n",
        "                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))"
      ],
      "metadata": {
        "id": "yeIOybDmXj7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN3D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN3D, self).__init__()\n",
        "\n",
        "        ## 3D CNN Layer\n",
        "        self.cnn_3d_layer = nn.Sequential(\n",
        "                                            nn.Conv3d(in_channels = 1, out_channels = 32, kernel_size = (5,5,5), stride = (2, 2, 2), padding = (0, 0, 0)), nn.BatchNorm3d(32), nn.ReLU(inplace=True), nn.Dropout3d(0.2),\n",
        "                                            nn.Conv3d(in_channels = 32, out_channels = 48, kernel_size = (3,3,3), stride = (2, 2, 2), padding = (0, 0, 0)), nn.BatchNorm3d(48), nn.ReLU(inplace=True), nn.Dropout3d(0.2),\n",
        "                                        )\n",
        "\n",
        "        ## compute conv1 & conv2 output shape to determine neurons in FFNN Layer\n",
        "        self.conv1_outshape = conv3D_output_size( (28, 256, 342), (0,0,0), (5,5,5), (2,2,2) )\n",
        "        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, (0,0,0), (3,3,3), (2,2,2))\n",
        "\n",
        "        ## FFNN Layer\n",
        "        self.ffnn_layer = nn.Sequential(\n",
        "                                          nn.Linear(48 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2], 256), nn.ReLU(inplace=True),\n",
        "                                          nn.Linear(256, 128), nn.ReLU(inplace=True), nn.dropout(0.2),\n",
        "                                          nn.Linear(128, 101)\n",
        "                                          )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # passing through 3d cnn layer\n",
        "        x = self.cnn_3d_layer(x)\n",
        "\n",
        "        # performing flattening\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # passing to FFNN layers\n",
        "        x = self.ffnn_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JdxxROPpXWkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "llYrK4XTQe9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3d cnn\n",
        "cnn3d = CNN3D().to(device)\n",
        "cnn3d.train()\n",
        "\n",
        "# defining optimizer\n",
        "optimizer = torch.optim.Adam(cnn3d.parameters(), lr=1e-4)\n",
        "\n",
        "# house keeping stuff\n",
        "epoch_train_losses = []\n",
        "epoch_train_scores = []"
      ],
      "metadata": {
        "id": "2kFPs9NJQf3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(15):\n",
        "\n",
        "    losses = []\n",
        "    scores = []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "\n",
        "\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device).view(-1, )\n",
        "\n",
        "        N_count += X.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn3d(X)  # output size = (batch, number of classes)\n",
        "\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # to compute accuracy\n",
        "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
        "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
        "        scores.append(step_score)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch + 1} [{N_count}/{len(train_loader.dataset)} ({round(100. * (batch_idx + 1)/ len(train_loader),2)}%)]\\tLoss: {round(loss.item(),5)}, Accu: {round(100 * step_score,2)}%')\n",
        "\n",
        "    epoch_train_losses.append(losses)\n",
        "    epoch_train_scores.append(scores)\n",
        ""
      ],
      "metadata": {
        "id": "ntu2aT5cQnLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}